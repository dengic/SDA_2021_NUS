{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 505 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (0.12.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (1.5.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (1.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from category_encoders) (1.19.2)\n",
      "Requirement already satisfied: six in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.21.1->category_encoders) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (2.1.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting toad\n",
      "  Downloading toad-0.0.64-cp38-cp38-macosx_10_14_x86_64.whl (14.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.2 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (1.5.2)\n",
      "Requirement already satisfied: setuptools in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (50.3.1.post20201107)\n",
      "Requirement already satisfied: numpy<1.20,>=1.18.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (1.19.2)\n",
      "Requirement already satisfied: pandas in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (1.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.12 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (0.17.0)\n",
      "Requirement already satisfied: Cython>=0.29.15 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (0.29.21)\n",
      "Requirement already satisfied: seaborn>=0.10.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from toad) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from pandas->toad) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from pandas->toad) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21->toad) (2.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from seaborn>=0.10.0->toad) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->toad) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn>=0.10.0->toad) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn>=0.10.0->toad) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn>=0.10.0->toad) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn>=0.10.0->toad) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/alansmac/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn>=0.10.0->toad) (1.3.0)\n",
      "Installing collected packages: toad\n",
      "Successfully installed toad-0.0.64\n"
     ]
    }
   ],
   "source": [
    "!pip install toad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from category_encoders.count import CountEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import toad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('train_label_new.csv', index_col=0)\n",
    "train_base = pd.read_csv('train_base_new.csv', index_col=0)\n",
    "train_trans = pd.read_csv('train_trans_new.csv', index_col=0)\n",
    "train_op = pd.read_csv('train_op_new.csv')\n",
    "train_op = train_op.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "test_base = pd.read_csv('test_base_new.csv', index_col=0)\n",
    "test_op = pd.read_csv('test_op_new.csv', index_col=0)\n",
    "test_trans = pd.read_csv('test_trans_new.csv', index_col=0)\n",
    "# test_label never show up to get rid of leakage problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>provider</th>\n",
       "      <th>level</th>\n",
       "      <th>...</th>\n",
       "      <th>product4_amount</th>\n",
       "      <th>product5_amount</th>\n",
       "      <th>product6_amount</th>\n",
       "      <th>product7_cnt</th>\n",
       "      <th>product7_fail_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_06800</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24877</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>...</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24706</td>\n",
       "      <td>24706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_23487</td>\n",
       "      <td>category 1</td>\n",
       "      <td>24895</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>...</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_36880</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24853</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>...</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_35392</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24938</td>\n",
       "      <td>category 1</td>\n",
       "      <td>category 1</td>\n",
       "      <td>...</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_35057</td>\n",
       "      <td>category 0</td>\n",
       "      <td>24956</td>\n",
       "      <td>category 0</td>\n",
       "      <td>category 2</td>\n",
       "      <td>...</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 0</td>\n",
       "      <td>level 1</td>\n",
       "      <td>24712</td>\n",
       "      <td>24706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user         sex    age    provider       level  ...  \\\n",
       "0  Train_06800  category 1  24877  category 0  category 2  ...   \n",
       "1  Train_23487  category 1  24895  category 0  category 2  ...   \n",
       "2  Train_36880  category 0  24853  category 0  category 2  ...   \n",
       "3  Train_35392  category 0  24938  category 1  category 1  ...   \n",
       "4  Train_35057  category 0  24956  category 0  category 2  ...   \n",
       "\n",
       "  product4_amount  product5_amount product6_amount  product7_cnt  \\\n",
       "0         level 0          level 0         level 1         24706   \n",
       "1         level 0          level 0         level 1         24712   \n",
       "2         level 0          level 0         level 1         24712   \n",
       "3         level 0          level 0         level 1         24712   \n",
       "4         level 0          level 0         level 1         24712   \n",
       "\n",
       "   product7_fail_cnt  \n",
       "0              24706  \n",
       "1              24706  \n",
       "2              24706  \n",
       "3              24706  \n",
       "4              24706  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 11)\n",
    "train_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>platform</th>\n",
       "      <th>tunnel_in</th>\n",
       "      <th>tunnel_out</th>\n",
       "      <th>...</th>\n",
       "      <th>ip</th>\n",
       "      <th>type2</th>\n",
       "      <th>ip_3</th>\n",
       "      <th>tm_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_13770</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days 09:02:45.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_13770</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days 09:03:58.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>46c69cbbce5f1568</td>\n",
       "      <td>b2e7fa260df4998d</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>...</td>\n",
       "      <td>f10a09fe9e522a47</td>\n",
       "      <td>11a213398ee0c623</td>\n",
       "      <td>ee386d6f9fe45d0d</td>\n",
       "      <td>18 days 11:06:49.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>42573d7287a8c9c2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 days 09:52:51.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_08351</td>\n",
       "      <td>42573d7287a8c9c2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ee790756007e69a</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 days 07:50:05.000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user          platform         tunnel_in        tunnel_out  ...  \\\n",
       "0  Train_13770  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a  ...   \n",
       "1  Train_13770  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a  ...   \n",
       "2  Train_08351  46c69cbbce5f1568  b2e7fa260df4998d  6ee790756007e69a  ...   \n",
       "3  Train_08351  42573d7287a8c9c2               NaN  6ee790756007e69a  ...   \n",
       "4  Train_08351  42573d7287a8c9c2               NaN  6ee790756007e69a  ...   \n",
       "\n",
       "                 ip             type2              ip_3  \\\n",
       "0               NaN  11a213398ee0c623               NaN   \n",
       "1               NaN  11a213398ee0c623               NaN   \n",
       "2  f10a09fe9e522a47  11a213398ee0c623  ee386d6f9fe45d0d   \n",
       "3               NaN               NaN               NaN   \n",
       "4               NaN               NaN               NaN   \n",
       "\n",
       "                      tm_diff  \n",
       "0  19 days 09:02:45.000000000  \n",
       "1  19 days 09:03:58.000000000  \n",
       "2  18 days 11:06:49.000000000  \n",
       "3  26 days 09:52:51.000000000  \n",
       "4  26 days 07:50:05.000000000  \n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 8)\n",
    "train_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate base and label\n",
    "train_df = train_base.copy()\n",
    "test_df = test_base.copy()\n",
    "train_df = train_label.merge(train_df, on=['user'], how='left')\n",
    "data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "# del train_base, test_base\n",
    "\n",
    "# concatenate train and test dataset of operation and transaction\n",
    "op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
    "trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
    "# del train_op, test_op, train_df, test_df, train_trans, test_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base = toad.detector.detect(data)\n",
    "# base"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Feature Engneering on base.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this feature have too many null values, so we drop this feature\n",
    "data.drop(['service3_level'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer into integer\n",
    "for col in ['balance', 'balance_avg', 'balance1', 'balance1_avg', 'balance2','balance2_avg', 'product1_amount', 'product2_amount',\n",
    "          'product3_amount', 'product4_amount', 'product5_amount', 'product6_amount']:\n",
    "    data[col] = data[col].apply(lambda x: int(x.split(' ')[1]) if type(x) != float else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cate = data.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  9.23it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 165.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Interactive item\n",
    "cate_features = ['sex', 'provider', 'level', 'verified', 'regist_type', 'agreement1', 'agreement2', 'agreement3', 'agreement4', \n",
    "                 'province', 'city', 'service3']\n",
    "\n",
    "for f1 in tqdm(cate_features):\n",
    "    for f2 in cate_features:\n",
    "        data['{}_{}'.format(f1, f2)] = data[f1] + '_' + data[f2]\n",
    "        \n",
    "for f in tqdm(cate_features):\n",
    "    data['{}_cnt'.format(f)] = data.groupby([f])['user'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# using Label Encoder to deal with categorical features\n",
    "for col in tqdm([col for col in original_cate  if col not in ['user']]):\n",
    "    le = LabelEncoder()\n",
    "    data[col].fillna('-1', inplace=True)\n",
    "    data[col] = le.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6b7707315342caaea9890409d1568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using Count Encoder to categorical features\n",
    "train_base = pd.read_csv('train_base_new.csv')\n",
    "test_base = pd.read_csv('test_base_new.csv')\n",
    "\n",
    "df = pd.concat([train_base,test_base], axis=0, ignore_index=True)\n",
    "# del train_base, test_base\n",
    "df_category=df.select_dtypes('object')\n",
    "\n",
    "df_category_nunique = df_category.nunique()\n",
    "A_cnt_features = [col for col in df_category_nunique.index if df_category_nunique.loc[col] > 5 and col!='user']\n",
    "\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    df_category[col] = df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = df_category[col].map(df_category[col].value_counts())\n",
    "    \n",
    "data=data.merge(frequency_fea,on=\"user\",how=\"left\")\n",
    "# del df,df_category,df_category_nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = ['age', 'using_time', 'card_a_cnt', 'card_b_cnt', 'card_c_cnt', 'card_d_cnt', 'op1_cnt', 'op2_cnt', 'service1_cnt', 'service1_amt', 'service2_cnt', \n",
    "                  'agreement_total', 'acc_count', 'login_cnt_period1', 'login_cnt_period2', 'ip_cnt', 'login_cnt_avg', 'login_days_cnt', 'balance', 'balance_avg', \n",
    "                  'balance1', 'balance1_avg', 'balance2', 'balance2_avg', 'product1_amount', 'product2_amount', 'product3_amount', 'product4_amount', 'product5_amount',\n",
    "                 'product6_amount', 'product7_cnt', 'product7_fail_cnt']\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "data[dense_features] = min_max.fit_transform(data[dense_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features from experience\n",
    "data['product7_fail_ratio'] = data['product7_fail_cnt'] / data['product7_cnt']\n",
    "data['city_count'] = data.groupby(['city'])['user'].transform('count')\n",
    "data['province_count'] = data.groupby(['province'])['user'].transform('count')\n",
    "\n",
    "data['card_cnt'] = data['card_a_cnt'] + data['card_b_cnt'] + data['card_c_cnt'] + data['card_d_cnt']\n",
    "\n",
    "data['acc_card_ratio'] = data['acc_count'] / data['card_cnt']\n",
    "data['login_cnt'] = data['login_cnt_period1'] + data['login_cnt_period2']\n",
    "\n",
    "data['login_cnt_period2_login_cnt_ratio'] = data['login_cnt_period2'] / data['login_cnt']\n",
    "data['login_cnt_period1_login_cnt_ratio'] = data['login_cnt_period1'] / data['login_cnt']\n",
    "\n",
    "data['using_time_op2_cnt_ratio'] = data['using_time'] / data['op2_cnt']\n",
    "data['using_time_op1_cnt_ratio'] = data['using_time'] / data['op1_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47782, 223)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('feature_base.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering on trans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform variable tm_diff\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second\n",
    "\n",
    "# extract elements from tm_diff\n",
    "# timestamp means how many seconds passed after the start point\n",
    "op_df['day'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "trans_df['day'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "trans_df['week'] = trans_df['day'].apply(lambda x: x % 7)\n",
    "op_df['min'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[1]))\n",
    "op_df['second'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[2]))\n",
    "\n",
    "# using timestamp to reorganize the sequence of each user's behaviour\n",
    "trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
    "op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
    "trans_df.reset_index(inplace=True, drop=True)\n",
    "op_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hour to group the behaviours in trans and op\n",
    "trans_df[\"time\"]=\"time\"\n",
    "trans_df.loc[trans_df.hour<=6,\"time\"]=\"night\"\n",
    "trans_df.loc[(trans_df.hour>6)&(trans_df.hour<=12),\"time\"]=\"morning\"\n",
    "trans_df.loc[(trans_df.hour>12)&(trans_df.hour<=18),\"time\"]=\"afternoon\"\n",
    "trans_df.loc[trans_df.hour>18,\"time\"]=\"evening\"\n",
    "\n",
    "op_df[\"time\"]=\"time\"\n",
    "op_df.loc[op_df.hour<=6,\"time\"]=\"night\"\n",
    "op_df.loc[(op_df.hour>6)&(op_df.hour<=12),\"time\"]=\"morning\"\n",
    "op_df.loc[(op_df.hour>12)&(op_df.hour<=18),\"time\"]=\"afternoon\"\n",
    "op_df.loc[op_df.hour>18,\"time\"]=\"evening\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features about transcation amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that calculate statistics about transaction amount\n",
    "def gen_user_amount_features(df):\n",
    "    group_df = df.groupby(['user'])['amount'].agg([\n",
    "        ('user_amount_mean', 'mean'),\n",
    "        ('user_amount_std','std'),\n",
    "        ('user_amount_max', 'max'),\n",
    "        ('user_amount_min', 'min'),\n",
    "        ('user_amount_sum', 'sum'),\n",
    "        ('user_amount_med', 'median'),\n",
    "        ('user_amount_cnt', 'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract features about transcation amount\n",
    "data = data.merge(gen_user_amount_features(trans_df), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  2.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# define unique value of each user\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg([\n",
    "        ('user_{}_{}_nuniq'.format(prefix, value), 'nunique')]\n",
    "    ).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract features from trans dataframe\n",
    "for col in tqdm(['day', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
    "    data = data.merge(gen_user_nunique_features(df=trans_df, value=col, prefix='trans'), on=['user'], how='left')\n",
    "    \n",
    "# transaction amount per day\n",
    "data['user_amount_per_days'] = data['user_amount_sum'] / data['user_trans_day_nuniq']\n",
    "# transcation amount each time\n",
    "data['user_amount_per_cnt'] = data['user_amount_sum'] / data['user_amount_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the transcation amount with each group\n",
    "def gen_user_group_amount_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='amount',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count', 'sum'])\n",
    "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "# group by platform\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='platform'), on=['user'], how='left')\n",
    "# gourp by type1\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='type1'), on=['user'], how='left')\n",
    "# group by type2\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='type2'), on=['user'], how='left')\n",
    "# group by time\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='time'), on=['user'], how='left')\n",
    "#group by week\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='week'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time axis, define a function to calculate users' transcation amount linked with feature 'days'\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['day']>window].groupby('user')['amount'].agg([\n",
    "        ('user_amount_mean_{}d'.format(window), 'mean'),\n",
    "        ('user_amount_std_{}d'.format(window),'std'),\n",
    "        ('user_amount_max_{}d'.format(window),'max'),\n",
    "        ('user_amount_min_{}d'.format(window), 'min'),\n",
    "        ('user_amount_sum_{}d'.format(window),'sum'),\n",
    "        ('user_amount_med_{}d'.format(window),'median'),\n",
    "        ('user_amount_cnt_{}d'.format(window),'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract amount feature within transcations after 7 days\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=7), on=['user'], how='left')\n",
    "# extract amount feature within transcations after 14 days\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=14), on=['user'], how='left')\n",
    "# extract amount feature within transcations after 21 days\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=21), on=['user'], how='left')\n",
    "# extract amount feature within transcations after 28 days\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=28), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate users' transcation amount linked with feature 'hours'\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['hour']>window].groupby('user')['amount'].agg([\n",
    "        ('user_amount_mean_{}h'.format(window), 'mean'),\n",
    "        ('user_amount_std_{}h'.format(window),'std'),\n",
    "        ('user_amount_max_{}h'.format(window),'max'),\n",
    "        ('user_amount_min_{}h'.format(window),'min'),\n",
    "        ('user_amount_sum_{}h'.format(window), 'sum'),\n",
    "        ('user_amount_med_{}h'.format(window), 'median'),\n",
    "        ('user_amount_cnt_{}h'.format(window), 'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract amount feature within transcations after 6 a.m.\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=6), on=['user'], how='left')\n",
    "# extract amount feature within transcations after 12 p.m.\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=12), on=['user'], how='left')\n",
    "# extract amount feature within transcations after 18 p.m.\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=18), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some features, we consider that null comes from some reason, so we generate features about missing value\n",
    "def gen_user_null_features(df, value, prefix):\n",
    "    df['is_null'] = 0\n",
    "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
    "\n",
    "    group_df = df.groupby(['user'])['is_null'].agg([('user_{}_{}_null_cnt'.format(prefix, value), 'sum'),\n",
    "                                                    ('user_{}_{}_null_ratio'.format(prefix, value),'mean')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract missing value and ratio within ip address\n",
    "data = data.merge(gen_user_null_features(df=trans_df, value='ip', prefix='trans'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first apperance of \"type1 == 45a1168437c708ff\"\n",
    "group_df = trans_df[trans_df['type1']=='45a1168437c708ff'].groupby(['user'])['day'].agg([('user_type1_45a1168437c708ff_min_day', 'min')]).reset_index()\n",
    "data = data.merge(group_df, on=['user'], how='left')\n",
    "del group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transaction amount of each users per day and hour\n",
    "def per_hour_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('per_hour_amt_sum','sum'),\n",
    "        ('per_hour_amt_mean','mean'),\n",
    "        ('per_hour_amt_max','max'),\n",
    "        ('per_hour_amt_min','min'),\n",
    "        ('per_hour_amt_cnt','count'),\n",
    "        ('per_hour_amt_std','std')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract transcation per day and hour features\n",
    "data = data.merge(per_hour_amt(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate each user's transcation amount group by tunnel_in and tunnel_out\n",
    "def tunnel_in_out_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('tunnel_in_out_amt_sum','sum'),\n",
    "        ('tunnel_in_out_amt_cnt','count'),\n",
    "        ('tunnel_in_out_amt_mean','mean'),\n",
    "        ('tunnel_in_out_amt_max','max'),\n",
    "        ('tunnel_in_out_amt_min','min'),\n",
    "        ('tunnel_in_out_amt_std','std'),\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract trascation amount group by tunnel_in and tunnel_out\n",
    "data = data.merge(tunnel_in_out_amt(trans_df,value1=\"tunnel_in\",value2=\"tunnel_out\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate each user's transcation amount group by \"ip\"\n",
    "def day_ip_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_ip_amt_sum','sum'),\n",
    "        ('day_ip_amt_mean','mean'),\n",
    "        ('day_ip_amt_std','std'),\n",
    "        ('day_ip_amt_max','max'),\n",
    "        ('day_ip_amt_min','min'),\n",
    "        ('day_ip_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract trascation amount group by \"ip\"\n",
    "data = data.merge(day_ip_amt(trans_df,value1=\"day\",value2=\"ip\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find the gap between transcations of each user for each hour\n",
    "def amt_gap(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['amount_gap']=abs(group_df[\"amount\"]-group_df[\"last_amount\"])\n",
    "    group_df=group_df[['user','amount_gap']]\n",
    "    group_df=group_df.groupby('user')['amount_gap'].agg([\n",
    "        ('amt_gap_sum','sum'),\n",
    "        ('amt_gap_mean','mean'),\n",
    "        ('amt_gap_std','std'),\n",
    "        ('amt_gap_max','max'),\n",
    "        ('amt_gap_min','min')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract features from the amount of transcation group by time gap\n",
    "data = data.merge(amt_gap(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the sum of gap ratio of amount for each user\n",
    "def gap_amt_rate(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2,'amount']]\n",
    "    group_df=group_df.groupby(['user', 'day', 'hour'])['amount'].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['gap_rate']=group_df['amount']/group_df['last_amount']\n",
    "    group_df=group_df[['user','gap_rate']]\n",
    "    group_df=group_df.groupby('user')['gap_rate'].agg('sum').reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract the gap ratio feature\n",
    "data = data.merge(gap_amt_rate(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the amount of transaction per day\n",
    "def per_day_amt(df,value):\n",
    "    group_df=df[['user',value, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('per_day_amt_mean','mean'),\n",
    "        ('per_day_amt_max','max'),\n",
    "        ('per_day_amt_min','min'),\n",
    "        ('per_day_amt_std','std')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract features using the function\n",
    "data = data.merge(per_day_amt(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find the gap between transcations of each user for each transcation day\n",
    "def day_amt_gap(df,value):\n",
    "    group_df=df[['user',value, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['amount_gap']=abs(group_df[\"amount\"]-group_df[\"last_amount\"])\n",
    "    group_df=group_df[['user','amount_gap']]\n",
    "    group_df=group_df.groupby('user')['amount_gap'].agg([\n",
    "        ('day_amt_gap_mean','mean'),\n",
    "        ('day_amt_gap_std','std'),\n",
    "        ('day_amt_gap_max','max'),\n",
    "        ('day_amt_gap_min','min')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract amount gap per day\n",
    "data = data.merge(day_amt_gap(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the gap ratio per day\n",
    "def day_gap_amt_rate(df,value):\n",
    "    group_df=df[['user',value,'amount']]\n",
    "    group_df=group_df.groupby(['user', value])['amount'].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['day_gap_rate']=group_df['amount']/group_df['last_amount']\n",
    "    group_df=group_df[['user','day_gap_rate']]\n",
    "    group_df=group_df.groupby('user')['day_gap_rate'].agg('sum').reset_index()\n",
    "    return group_df\n",
    "\n",
    "# generate amount gap ratio per day\n",
    "data = data.merge(day_gap_amt_rate(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the type1 transcation per day\n",
    "def day_type_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_type1_amt_sum','sum'),\n",
    "        ('day_type1_amt_mean','mean'),\n",
    "        ('day_type1_amt_std','std'),\n",
    "        ('day_type1_amt_max','max'),\n",
    "        ('day_type1_amt_min','min'),\n",
    "        ('day_type1_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract type1 transcation amount per day\n",
    "data = data.merge(day_type_amt(trans_df,value1=\"day\",value2=\"type1\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the type2 transcation per day\n",
    "def day_type2_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_type2_amt_sum','sum'),\n",
    "        ('day_type2_amt_mean','mean'),\n",
    "        ('day_type2_amt_std','std'),\n",
    "        ('day_type2_amt_max','max'),\n",
    "        ('day_type2_amt_min','min'),\n",
    "        ('day_type2_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract type2 transcation amount per day\n",
    "data = data.merge(day_type2_amt(trans_df,value1=\"day\",value2=\"type2\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>day_type2_amt_std</th>\n",
       "      <th>day_type2_amt_max</th>\n",
       "      <th>day_type2_amt_min</th>\n",
       "      <th>day_type2_amt_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.227586</td>\n",
       "      <td>...</td>\n",
       "      <td>129348.034147</td>\n",
       "      <td>321635.0</td>\n",
       "      <td>54093.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.252414</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36098.0</td>\n",
       "      <td>36098.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.354483</td>\n",
       "      <td>...</td>\n",
       "      <td>30670.951279</td>\n",
       "      <td>221240.0</td>\n",
       "      <td>162423.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_00005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176552</td>\n",
       "      <td>...</td>\n",
       "      <td>70341.901890</td>\n",
       "      <td>267494.0</td>\n",
       "      <td>39630.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_00006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.219310</td>\n",
       "      <td>...</td>\n",
       "      <td>90202.285063</td>\n",
       "      <td>339168.0</td>\n",
       "      <td>34914.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  label  sex       age  ...  day_type2_amt_std  \\\n",
       "0  Train_00000    0.0    1  0.227586  ...      129348.034147   \n",
       "1  Train_00001    1.0    1  0.252414  ...                NaN   \n",
       "2  Train_00002    0.0    1  0.354483  ...       30670.951279   \n",
       "3  Train_00005    0.0    1  0.176552  ...       70341.901890   \n",
       "4  Train_00006    0.0    1  0.219310  ...       90202.285063   \n",
       "\n",
       "   day_type2_amt_max  day_type2_amt_min  day_type2_amt_cnt  \n",
       "0           321635.0            54093.0                4.0  \n",
       "1            36098.0            36098.0                1.0  \n",
       "2           221240.0           162423.0                3.0  \n",
       "3           267494.0            39630.0               12.0  \n",
       "4           339168.0            34914.0                9.0  \n",
       "\n",
       "[5 rows x 435 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trans word2vec ...\n",
      "Start gen feat of platform ...\n",
      "Start gen feat of tunnel_in ...\n",
      "Start gen feat of tunnel_out ...\n",
      "Start gen feat of amount ...\n",
      "Start gen feat of type1 ...\n",
      "Start gen feat of type2 ...\n",
      "Start gen feat of ip ...\n",
      "Start gen feat of day ...\n",
      "Start gen feat of hour ...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "def w2v_feat(data_frame, feat, mode):\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True)\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=5, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_trans_{t}_{n}' for n in range(5)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "# generate word2vec features\n",
    "trans_feat=[\"platform\",\"tunnel_in\",\"tunnel_out\",\"amount\",\"type1\",\"type2\",\"ip\",\"day\",\"hour\"]\n",
    "data=data.merge(w2v_feat(trans_df,trans_feat,'trans'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47782, 615)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820ac42da5c45bc99743c6ffb8ac41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fadea74208046719580005a7957c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using Count Encoder to categorical features in df_trans\n",
    "from category_encoders.count import CountEncoder\n",
    "for i in [\"ip\",\"ip_3\",\"amount\"]:\n",
    "    trans_df[\"count_{}_trans\".format(i)]=CountEncoder().fit_transform(trans_df[i])\n",
    "    group_df=trans_df.groupby('user')[\"count_{}_trans\".format(i)].agg([\n",
    "        (\"count_{}_trans_max\".format(i),'max'),\n",
    "        (\"count_{}_trans_min\".format(i), 'min'),\n",
    "        (\"count_{}_trans_mean\".format(i),'mean'),\n",
    "        (\"count_{}_trans_std\".format(i),'std')\n",
    "    ]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")\n",
    "    \n",
    "trans_df.drop(columns=trans_df.columns[-3:],inplace=True)\n",
    "\n",
    "# extract frequncy features for features which have more than five unique values\n",
    "trans_df_category=trans_df.drop(columns=[\"tm_diff\",\"time\"])\n",
    "trans_df_category_nunique = trans_df_category.nunique()\n",
    "A_cnt_features = [col for col in trans_df_category_nunique.index if trans_df_category_nunique.loc[col] > 5 and col!='user']\n",
    "# print(len(A_cnt_features))\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = trans_df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    trans_df_category[col] = trans_df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = trans_df_category[col].map(trans_df_category[col].value_counts())\n",
    "    \n",
    "for i in tqdm_notebook(frequency_fea.columns[1:]):\n",
    "    group_df=frequency_fea[[\"user\",i]]\n",
    "    group_df=group_df.groupby(\"user\")[i].agg([\n",
    "        ('freq_{}_max'.format(i),'max'),\n",
    "        ('freq_{}_min'.format(i),'min'),\n",
    "        ('freq_{}_mean'.format(i),'mean'),\n",
    "        ('freq_{}_std'.format(i),'std')]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47782, 671)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from op.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>op_type</th>\n",
       "      <th>op_mode</th>\n",
       "      <th>op_device</th>\n",
       "      <th>...</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>second</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>87ee0bdf333a54da</td>\n",
       "      <td>4ff62c2e280249dc</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>6b0823f22acf82cf</td>\n",
       "      <td>4ff62c2e280249dc</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>3089f3f91de53eee</td>\n",
       "      <td>3089f3f91de53eee</td>\n",
       "      <td>4ff62c2e280249dc</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>b26bc49195bd79cf</td>\n",
       "      <td>6b0823f22acf82cf</td>\n",
       "      <td>4ff62c2e280249dc</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>3089f3f91de53eee</td>\n",
       "      <td>3089f3f91de53eee</td>\n",
       "      <td>4ff62c2e280249dc</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user           op_type           op_mode         op_device  ...  \\\n",
       "0  Train_00000  b26bc49195bd79cf  87ee0bdf333a54da  4ff62c2e280249dc  ...   \n",
       "1  Train_00000  b26bc49195bd79cf  6b0823f22acf82cf  4ff62c2e280249dc  ...   \n",
       "2  Train_00000  3089f3f91de53eee  3089f3f91de53eee  4ff62c2e280249dc  ...   \n",
       "3  Train_00000  b26bc49195bd79cf  6b0823f22acf82cf  4ff62c2e280249dc  ...   \n",
       "4  Train_00000  3089f3f91de53eee  3089f3f91de53eee  4ff62c2e280249dc  ...   \n",
       "\n",
       "  hour min second       time  \n",
       "0   13   4     34  afternoon  \n",
       "1   13   4     45  afternoon  \n",
       "2   13   4     45  afternoon  \n",
       "3   13   4     55  afternoon  \n",
       "4   13   4     55  afternoon  \n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_tfidf_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    # use SVD method to reduce the dimension of this sparse matrix\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=623)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_type'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_countvec_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=623)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_type'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start op word2vec ...\n",
      "Start gen feat of op_type ...\n",
      "Start gen feat of op_mode ...\n",
      "Start gen feat of op_device ...\n",
      "Start gen feat of ip ...\n",
      "Start gen feat of channel ...\n",
      "Start gen feat of day ...\n",
      "Start gen feat of hour ...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "def w2v_feat(data_frame, feat, mode):\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True)\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=5, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_op_{t}_{n}' for n in range(5)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "#生成word2vec特征\n",
    "op_feat=[\"op_type\",\"op_mode\",\"op_device\",\"ip\",\"channel\",\"day\",\"hour\"]\n",
    "data=data.merge(w2v_feat(op_df,op_feat,'op'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Counter Encoder to generate features from op_df\n",
    "from category_encoders.count import CountEncoder\n",
    "for i in [\"ip\",\"ip_3\",\"op_device\",\"op_type\",\"op_mode\"]:\n",
    "    op_df[\"count_{}_op\".format(i)]=CountEncoder().fit_transform(op_df[i])\n",
    "    group_df=op_df.groupby('user')[\"count_{}_op\".format(i)].agg([\n",
    "        (\"count_{}_op_max\".format(i),'max'),\n",
    "        (\"count_{}_op_min\".format(i), 'min'),\n",
    "        (\"count_{}_op_mean\".format(i),'mean'),\n",
    "        (\"count_{}_op_std\".format(i),'std')\n",
    "    ]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")\n",
    "    \n",
    "op_df.drop(columns=op_df.columns[-5:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7d7719f0f34bdaaa702027d951416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c234a6ed7f3449aa0a7bcb892e764cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# encode the frequency of opeartion and calculate statistics\n",
    "op_df_category=op_df.drop(columns=[\"tm_diff\",\"time\"])\n",
    "op_df_category_nunique = op_df_category.nunique()\n",
    "A_cnt_features = [col for col in op_df_category_nunique.index if op_df_category_nunique.loc[col] > 5 and col!='user']\n",
    "print(len(A_cnt_features))\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = op_df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    op_df_category[col] = op_df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = op_df_category[col].map(op_df_category[col].value_counts())\n",
    "\n",
    "for i in tqdm_notebook(frequency_fea.columns[1:]):\n",
    "    group_df=frequency_fea[[\"user\",i]]\n",
    "    group_df=group_df.groupby(\"user\")[i].agg([\n",
    "       ('freq_{}_max'.format(i),'max'),\n",
    "        ('freq_{}_min'.format(i),'min'),\n",
    "        ('freq_{}_mean'.format(i),'mean'),\n",
    "        ('freq_{}_std'.format(i),'std')]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# define a function to calculate the possible values of each feature for each user\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg([\n",
    "        ('user_{}_{}_nuniq'.format(prefix, value),'nunique')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "# extract number of unique values of each user\n",
    "for col in tqdm(['op_type', 'op_mode', 'ip', 'channel', 'ip_3', 'day']):\n",
    "    data = data.merge(gen_user_nunique_features(df=op_df, value=col, prefix='op'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count how many times of one operation type happens group by day\n",
    "def gen_user_window_op_features(df, window):\n",
    "    group_df = df[df['day']>window].groupby('user')['op_type'].agg([\n",
    "        ('user_op_cnt_{}d'.format(window),'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "op_df[\"day\"]=op_df[\"day\"].astype(int)\n",
    "\n",
    "# extract operation type counting after 5 days\n",
    "data = data.merge(gen_user_window_op_features(df=op_df, window=5), on=['user'], how='left')\n",
    "# extract operation type counting after 10 days\n",
    "data = data.merge(gen_user_window_op_features(df=op_df, window=10), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count how many times of one operation type happens group by hour\n",
    "def gen_op_window_hour_features(df, window):\n",
    "    group_df = df[df['hour']>window].groupby('user')['op_type'].agg([\n",
    "        ('user_op_cnt_{}h'.format(window),'count')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "op_df[\"hour\"]=op_df[\"hour\"].astype(int)\n",
    "# extract operation type counting after 6\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=6), on=['user'], how='left')\n",
    "# extract operation type counting after 12\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=12), on=['user'], how='left')\n",
    "# extract operation type counting after 18\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=18), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count how many times of one operation type happens group by time(i.e morning, afternoon, evening and night)\n",
    "def gen_user_group_op_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='op_type',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count'])\n",
    "    group_df.columns = ['user_{}_{}_op_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "# extrate operation type counts group by time\n",
    "data = data.merge(gen_user_group_op_features(df=op_df, value='time'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting how many times each user operated the app\n",
    "op_count = op_df[['user']]\n",
    "op_count['op_count'] = 1\n",
    "op_count = op_count.groupby('user').agg('count').reset_index()\n",
    "data = pd.merge(data, op_count, on='user', how='left')\n",
    "# del op_count\n",
    "\n",
    "# calculate how many times of operation happened per day\n",
    "data[\"op_cnt_per_day\"]=data[\"op_count\"]/data[\"user_op_day_nuniq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the statistics of the counting of operations of each user in per hour per day\n",
    "def day_per_hour_cnt(df,value1,value2):\n",
    "    group_df = op_df[['user', 'day', 'hour']]\n",
    "    group_df['everyday_everyhour'] = 1\n",
    "    group_df = group_df.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "    group_df = group_df.drop(['day', 'hour'],axis = 1)\n",
    "    group_df = group_df.groupby('user')['everyday_everyhour'].agg([\n",
    "        ('day_per_hour_mean','mean'),\n",
    "        ('day_per_hour_max','max'),\n",
    "        ('day_per_hour_min','min'), \n",
    "        ('day_per_hour_std','std')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "data = data.merge(day_per_hour_cnt(op_df,\"day\",\"hour\"), on='user', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the statistics of the counting of operations of each user in per day\n",
    "frequence_one_day = op_df[['user', 'day']]\n",
    "frequence_one_day['everyday'] = 1\n",
    "frequence_one_day = frequence_one_day.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day = frequence_one_day.drop('day', axis=1)\n",
    "frequence_one_day = frequence_one_day.groupby('user')['everyday'].agg([\n",
    "    ('per_day_mean','mean'),\n",
    "    ('per_day_max','max'),\n",
    "    ('per_day_min','min'),\n",
    "    ('per_day_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_day, on='user', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the opeartions happened in the morning for each user\n",
    "frequence_morning = op_df[op_df.time==\"morning\"][['user', 'day','hour']]\n",
    "frequence_morning['everyday_morning'] = 1\n",
    "frequence_morning = frequence_morning.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_morning = frequence_morning.groupby(['user', 'day'])['everyday_morning'].agg('sum').reset_index()\n",
    "frequence_morning = frequence_morning[['user', 'everyday_morning']]\n",
    "frequence_morning = frequence_morning.groupby('user')['everyday_morning'].agg([\n",
    "    ('per_mor_mean','mean'),\n",
    "    ('per_mor_max','max'),\n",
    "    ('per_mor_min','min'),\n",
    "    ('per_mor_std','std')]).reset_index()\n",
    "data = data.merge(frequence_morning, on='user', how='left')\n",
    "del frequence_morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the opeartions happened in the afternoon for each user\n",
    "frequence_afternoon = op_df[op_df.time==\"afternoon\"][['user', 'day','hour']]\n",
    "frequence_afternoon['everyday_afternoon'] = 1\n",
    "frequence_afternoon = frequence_afternoon.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_afternoon = frequence_afternoon.groupby(['user', 'day'])['everyday_afternoon'].agg('sum').reset_index()\n",
    "frequence_afternoon = frequence_afternoon[['user', 'everyday_afternoon']]\n",
    "frequence_afternoon = frequence_afternoon.groupby('user')['everyday_afternoon'].agg([\n",
    "    ('per_after_mean','mean'),\n",
    "    ('per_after_max','max'),\n",
    "    ('per_after_min','min'),\n",
    "    ('per_after_std','std')]).reset_index()\n",
    "data = data.merge(frequence_afternoon, on='user', how='left')\n",
    "del frequence_afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the opeartions happened in the evening for each user\n",
    "frequence_evening = op_df[op_df.time==\"evening\"][['user', 'day','hour']]\n",
    "frequence_evening['everyday_evening'] = 1\n",
    "frequence_evening = frequence_evening.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_evening = frequence_evening.groupby(['user', 'day'])['everyday_evening'].agg('sum').reset_index()\n",
    "frequence_evening = frequence_evening[['user', 'everyday_evening']]\n",
    "frequence_evening = frequence_evening.groupby('user')['everyday_evening'].agg([\n",
    "    ('per_eve_mean','mean'),\n",
    "    ('per_eve_max','max'),\n",
    "    ('per_eve_min','min'),\n",
    "    ('per_eve_std','std')]).reset_index()\n",
    "data = data.merge(frequence_evening, on='user', how='left')\n",
    "del frequence_evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the opeartions happened at night for each user\n",
    "frequence_night = op_df[op_df.time==\"night\"][['user', 'day','hour']]\n",
    "frequence_night['everyday_night'] = 1\n",
    "frequence_night = frequence_night.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_night = frequence_night.groupby(['user', 'day'])['everyday_night'].agg('sum').reset_index()\n",
    "frequence_night = frequence_night[['user', 'everyday_night']]\n",
    "frequence_night = frequence_night.groupby('user')['everyday_night'].agg([\n",
    "    ('per_night_mean','mean'),\n",
    "    ('per_night_max','max'),\n",
    "    ('per_night_min','min'),\n",
    "    ('per_night_std','std')]).reset_index()\n",
    "data = data.merge(frequence_night, on='user', how='left')\n",
    "del frequence_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the operation counting between two days\n",
    "frequence_one_day_gap = op_df[['user', 'day']]\n",
    "frequence_one_day_gap['everyday'] = 1\n",
    "frequence_one_day_gap = frequence_one_day_gap.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day_gap['everyday_before'] = frequence_one_day_gap.groupby('user')['everyday'].shift(1)\n",
    "frequence_one_day_gap['everyday_before_gap'] = abs(frequence_one_day_gap['everyday'] - frequence_one_day_gap['everyday_before'])\n",
    "frequence_one_day_gap = frequence_one_day_gap[['user', 'everyday_before_gap']].groupby('user')['everyday_before_gap'].agg([\n",
    "    ('op_day_gap_mean','mean'),\n",
    "    ('op_day_gap_max','max'),\n",
    "    ('op_day_gap_min','min'),\n",
    "    ('op_day_gap_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_day_gap, on='user', how='left')\n",
    "del frequence_one_day_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the operation counting between two hours\n",
    "frequence_one_hour_gap = op_df[['user', 'day', 'hour']]\n",
    "frequence_one_hour_gap['everyhour'] = 1\n",
    "frequence_one_hour_gap = frequence_one_hour_gap.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_one_hour_gap['everyhour_before'] = frequence_one_hour_gap.groupby('user')['everyhour'].shift(1)\n",
    "frequence_one_hour_gap['everyhour_before_gap'] = abs(frequence_one_hour_gap['everyhour'] - frequence_one_hour_gap['everyhour_before'])\n",
    "frequence_one_hour_gap = frequence_one_hour_gap[['user', 'everyhour_before_gap']].groupby('user')['everyhour_before_gap'].agg([\n",
    "    ('hour_gap_mean','mean'),\n",
    "    ('hour_gap_max','max'),\n",
    "    ('hour_gap_min','min'),\n",
    "    ('hour_gap_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_hour_gap, on='user', how='left')\n",
    "del frequence_one_hour_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of the number of the operations between two days\n",
    "frequence_one_day_rate = op_df[['user', 'day']]\n",
    "frequence_one_day_rate['everyday'] = 1\n",
    "frequence_one_day_rate = frequence_one_day_rate.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day_rate['everyday_before'] = frequence_one_day_rate.groupby('user')['everyday'].shift(1)\n",
    "frequence_one_day_rate['everyday_before_rate'] = frequence_one_day_rate['everyday'] / frequence_one_day_rate['everyday_before']\n",
    "frequence_one_day_rate = frequence_one_day_rate[['user', 'everyday_before_rate']].groupby('user')['everyday_before_rate'].agg('sum').reset_index()\n",
    "data = data.merge(frequence_one_day_rate, on='user', how='left')\n",
    "del frequence_one_day_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of the number of the operations between two hours\n",
    "frequence_one_hour_rate = op_df[['user', 'day', 'hour']]\n",
    "frequence_one_hour_rate['everyhour'] = 1\n",
    "frequence_one_hour_rate = frequence_one_hour_rate.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_one_hour_rate['everyhour_before'] = frequence_one_hour_rate.groupby('user')['everyhour'].shift(1)\n",
    "frequence_one_hour_rate['everyhour_before_rate'] = frequence_one_hour_rate['everyhour'] - frequence_one_hour_rate['everyhour_before']\n",
    "frequence_one_hour_rate = frequence_one_hour_rate[['user', 'everyhour_before_rate']].groupby('user')['everyhour_before_rate'].agg('sum').reset_index()\n",
    "data = data.merge(frequence_one_hour_rate, on='user', how='left')\n",
    "del frequence_one_hour_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of operatins of each user happened per second\n",
    "every_second = op_df[['user', 'second']]\n",
    "every_second['operation_second_op'] = 1\n",
    "every_second = every_second.groupby(['user', 'second']).agg('count').reset_index()\n",
    "every_second = every_second[['user', 'operation_second_op']]\n",
    "every_second = every_second.groupby('user')['operation_second_op'].agg([\n",
    "    ('per_sec_max','max'),\n",
    "    ('per_sec_min','min'),\n",
    "    ('per_sec_mean','mean'),\n",
    "    ('per_sec_std','std')]).reset_index()\n",
    "data = data.merge(every_second, on='user', how='left')\n",
    "del every_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the statistics of the number of operatins of each user happened per minutes\n",
    "every_minute = op_df[['user', 'min']]\n",
    "every_minute['operation_minute_op'] = 1\n",
    "every_minute = every_minute.groupby(['user', 'min']).agg('count').reset_index()\n",
    "every_minute = every_minute[['user', 'operation_minute_op']]\n",
    "every_minute = every_minute.groupby('user')['operation_minute_op'].agg([\n",
    "    ('per_minute_max','max'),\n",
    "    ('per_minute_min','min'),\n",
    "    ('per_minute_mean','mean'),\n",
    "    ('per_minute_std','std')]).reset_index()\n",
    "data = data.merge(every_minute, on='user', how='left')\n",
    "del every_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of different devices one may use per day\n",
    "dev_per_day_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"op_device\"]].drop_duplicates()\n",
    "dev_per_day_cnt=pd.DataFrame(dev_per_day_cnt.groupby([\"user\",\"day\"])[\"op_device\"].nunique()).reset_index()\n",
    "dev_per_day_cnt=dev_per_day_cnt[[\"user\",\"op_device\"]]\n",
    "dev_per_day_cnt=dev_per_day_cnt.groupby(['user'])[\"op_device\"].agg([\n",
    "    (\"dev_nun_mean\",\"mean\"),\n",
    "    (\"dev_nun_max\",\"max\"),\n",
    "    (\"dev_nun_min\",\"min\"),\n",
    "    (\"dev_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(dev_per_day_cnt, on='user', how='left')\n",
    "del dev_per_day_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of different devices one may use per hour\n",
    "dev_per_hour_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"hour\",\"op_device\"]].drop_duplicates()\n",
    "dev_per_hour_cnt=pd.DataFrame(dev_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"op_device\"].nunique()).reset_index()\n",
    "dev_per_hour_cnt=dev_per_hour_cnt[[\"user\",\"op_device\"]]\n",
    "dev_per_hour_cnt=dev_per_hour_cnt.groupby(['user'])[\"op_device\"].agg([\n",
    "    (\"dev_hour_nun_mean\",\"mean\"),\n",
    "    (\"dev_hour_nun_max\",\"max\"),\n",
    "    (\"dev_hour_nun_min\",\"min\"),\n",
    "    (\"dev_hour_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(dev_per_hour_cnt, on='user', how='left')\n",
    "del dev_per_hour_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of different ip one may use per day\n",
    "ip_per_day_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"ip\"]].drop_duplicates()\n",
    "ip_per_day_cnt=pd.DataFrame(ip_per_day_cnt.groupby([\"user\",\"day\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_day_cnt=ip_per_day_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_day_cnt=ip_per_day_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_nun_mean\",\"mean\"),\n",
    "    (\"ip_nun_max\",\"max\"),\n",
    "    (\"ip_nun_mean\",\"mean\"),\n",
    "    (\"ip_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_day_cnt, on='user', how='left')\n",
    "del ip_per_day_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of different ip one may use per hour\n",
    "ip_per_hour_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"ip\"]].drop_duplicates()\n",
    "ip_per_hour_cnt=pd.DataFrame(ip_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_hour_cnt=ip_per_hour_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_hour_cnt=ip_per_hour_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_hour_nun_max\",\"max\"),\n",
    "    (\"ip_hour_nun_min\",\"min\"),\n",
    "    (\"ip_hour_nun_mean\",\"mean\"),\n",
    "    (\"ip_hour_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_hour_cnt, on='user', how='left')\n",
    "del ip_per_hour_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistics of the number of different ip one may use per day per minute\n",
    "ip_per_min_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"min\",\"ip\"]].drop_duplicates()\n",
    "ip_per_min_cnt=pd.DataFrame(ip_per_min_cnt.groupby([\"user\",\"day\",\"hour\",\"min\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_min_cnt=ip_per_min_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_min_cnt=ip_per_min_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_min_nun_max\",\"max\"),\n",
    "    (\"ip_min_nun_min\",\"min\"),\n",
    "    (\"ip_min_nun_mean\",\"mean\"),\n",
    "    (\"ip_min_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_min_cnt, on='user', how='left')\n",
    "del ip_per_min_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47782, 994)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the generate features\n",
    "data.to_csv(\"data_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from gensim.models import Word2Vec\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 623\n",
    "\n",
    "# read_data\n",
    "df_train_label = pd.read_csv('train_label_new.csv', index_col=0)\n",
    "df_train_base = pd.read_csv('train_base_new.csv', index_col=0)\n",
    "df_train_trans = pd.read_csv('train_trans_new.csv', index_col=0)\n",
    "df_train_op = pd.read_csv('train_op_new.csv')\n",
    "df_train_op = df_train_op.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_test_base = pd.read_csv('test_base_new.csv', index_col=0)\n",
    "df_test_trans = pd.read_csv('test_trans_new.csv', index_col=0)\n",
    "df_test_op = pd.read_csv('test_op_new.csv', index_col=0)\n",
    "\n",
    "df_trans = df_train_trans.append(df_test_trans)\n",
    "df_trans = df_trans.reset_index(drop=True)\n",
    "\n",
    "df_op = df_train_op.append(df_test_op)\n",
    "df_op = df_op.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(tm):\n",
    "    days, _, time = tm.split(' ')\n",
    "    time = time.split('.')[0]\n",
    "\n",
    "    time = '2020-1-1 ' + time\n",
    "    time = datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "    time = (time + timedelta(days=int(days)))\n",
    "\n",
    "    return time\n",
    "\n",
    "\n",
    "df_trans['date'] = df_trans['tm_diff'].apply(parse_time)\n",
    "df_trans['day'] = df_trans['date'].dt.day\n",
    "df_trans['hour'] = df_trans['date'].dt.hour\n",
    "\n",
    "df_op['date'] = df_op['tm_diff'].apply(parse_time)\n",
    "df_op['day'] = df_op['date'].dt.day\n",
    "df_op['hour'] = df_op['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.sort_values(['user', 'date'], inplace=True)\n",
    "df_trans = df_trans.reset_index(drop=True)\n",
    "\n",
    "df_op.sort_values(['user', 'date'], inplace=True)\n",
    "df_op = df_op.reset_index(drop=True)\n",
    "\n",
    "df_train = df_train_base.merge(df_train_label, how='left')\n",
    "df_test = df_test_base\n",
    "\n",
    "df_feature = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('embedding', exist_ok=True)\n",
    "\n",
    "\n",
    "def w2v_emb(df, f1, f2, prefix):\n",
    "    emb_size = 32\n",
    "\n",
    "    model_path = 'model/{}_w2v_{}_{}_{}.m'.format(prefix, f1, f2, emb_size)\n",
    "    embedding_path = 'embedding/{}_{}_{}_{}.pkl'.format(prefix, f1, f2, emb_size)\n",
    "\n",
    "    if os.path.exists(embedding_path):\n",
    "        embedding = pd.read_pickle(embedding_path)\n",
    "        return embedding\n",
    "\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg(\n",
    "        {'{}_{}_list'.format(f1, f2): list})\n",
    "\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        model = Word2Vec(sentences,\n",
    "                         size=emb_size,\n",
    "                         window=5,\n",
    "                         min_count=5,\n",
    "                         sg=0,\n",
    "                         hs=1,\n",
    "                         seed=seed)\n",
    "        model.save(model_path)\n",
    "\n",
    "    emb_matrix = []\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "\n",
    "    df_emb = pd.DataFrame(emb_matrix)\n",
    "    df_emb.columns = [\n",
    "        '{}_{}_{}_emb_{}'.format(prefix, f1, f2, i) for i in range(emb_size)\n",
    "    ]\n",
    "\n",
    "    embedding = pd.concat([tmp, df_emb], axis=1)\n",
    "    embedding.to_pickle(embedding_path)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_emb(df, f1, f2, prefix):\n",
    "    emb_size = 32\n",
    "\n",
    "    df[f2] = df[f2].astype(str)\n",
    "    df[f2].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby([f1]).apply(\n",
    "        lambda x: x[f2].tolist()).reset_index()\n",
    "    group_df.columns = [f1, 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['{}_svd_tfidf_{}_{}'.format(prefix,\n",
    "        f2, i) for i in range(emb_size)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countvec_emb(df, f1, f2):\n",
    "    emb_size = 32\n",
    "\n",
    "    df[f2] = df[f2].astype(str)\n",
    "    df[f2].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby([f1]).apply(\n",
    "        lambda x: x[f2].tolist()).reset_index()\n",
    "    group_df.columns = [f1, 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(\n",
    "        f2, i) for i in range(emb_size)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['date_diff'] = df_op.groupby('user')['date'].diff()\n",
    "df_op['op_second_diff'] = df_op['date_diff'].dt.seconds\n",
    "df_op['op_hour_diff'] = df_op['op_second_diff'] / 3600\n",
    "df_op['op_day_diff'] = df_op['op_hour_diff'] / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.15s/it]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.83it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for window in [15, 3, 5]:\n",
    "    for col in tqdm(['op_type', 'op_mode', 'net_type', 'channel', 'hour']):\n",
    "        df_temp = df_op[df_op['day'] > 15 - window][['user', col]].copy()\n",
    "        df_temp['tmp'] = 1\n",
    "        df_temp = df_temp.pivot_table(index='user', columns=col,\n",
    "                                      values='tmp', aggfunc=np.sum).reset_index().fillna(0)\n",
    "        df_temp.columns = [c if c == 'user' else 'op_{}_{}_count_{}d'.format(\n",
    "            col, c, window) for c in df_temp.columns]\n",
    "        df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(['op_type', 'op_mode', 'net_type', 'channel']):\n",
    "    df_temp = df_op[['user', 'hour', col]].copy()\n",
    "    df_temp = df_temp.pivot_table(index='user', columns=col,\n",
    "                                  values='hour', aggfunc=['mean', 'std', 'max', 'min']).fillna(0)\n",
    "    df_temp.columns = ['op_{}_{}_hour_{}'.format(col, f[1], f[0]) for f in df_temp.columns]\n",
    "    df_temp.reset_index(inplace=True)\n",
    "    df_temp.rename({'index': 'user'}, inplace=True, axis=1)\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_op.groupby(['user', 'op_device']).size().reset_index()\n",
    "df_temp.drop([0], axis=1, inplace=True)\n",
    "df_temp = df_temp.sort_values(\n",
    "    by=['user', 'op_device'], ascending=['asc', 'asc'])\n",
    "df_temp.drop_duplicates('user', keep='last', inplace=True)\n",
    "df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['hour', 'day', 'op_second_diff']:\n",
    "    df_temp = df_op.groupby('user')[f].agg([\n",
    "        ('op_{}_mean'.format(f), 'mean'), \n",
    "        ('op_{}_std'.format(f), 'std'), \n",
    "        ('op_{}_max'.format(f), 'max'),\n",
    "        ('op_{}_min'.format(f), 'min')\n",
    "    ]).reset_index()\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans['date_diff'] = df_trans.groupby('user')['date'].diff()\n",
    "df_trans['trans_second_diff'] = df_trans['date_diff'].dt.seconds\n",
    "df_trans['trans_hour_diff'] = df_trans['trans_second_diff'] / 3600\n",
    "df_trans['trans_day_diff'] = df_trans['trans_hour_diff'] / 24\n",
    "\n",
    "df_trans['amount'] = np.log1p(df_trans['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(['platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'hour']):\n",
    "    df_temp = df_trans.pivot_table(\n",
    "        index='user', columns=col, values='amount', aggfunc=['sum', 'mean', 'max', 'min', 'std', 'median']).fillna(0)\n",
    "    df_temp.columns = ['trans_{}_{}_amount_{}'.format(col, f[1], f[0]) for f in df_temp.columns]\n",
    "    df_temp.reset_index(inplace=True)\n",
    "    df_temp.rename({'index': 'user'}, inplace=True, axis=1)\n",
    "\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for window in tqdm([31, 1, 3, 5, 7, 10, 15]):\n",
    "    df_temp = df_trans[df_trans['day'] > 31-window].groupby('user')['amount'].agg([\n",
    "        ('trans_amount_mean_{}d'.format(window), 'mean'),\n",
    "        ('trans_amount_std_{}d'.format(window), 'std'),\n",
    "        ('trans_amount_max_{}d'.format(window), 'max'),\n",
    "        ('trans_amount_min_{}d'.format(window), 'min'),\n",
    "        ('trans_amount_sum_{}d'.format(window), 'sum'),\n",
    "    ]).reset_index()\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [3, 5, 10]:\n",
    "    for col in ['type1', 'type2']:\n",
    "        df_temp = df_trans[df_trans['day'] > 31 - window].pivot_table(\n",
    "            index='user', columns=col, values='amount', aggfunc=['sum']).fillna(0)\n",
    "        df_temp.columns = ['trans_{}_{}_amount_{}_{}d'.format(col, f[1], f[0], window) for f in df_temp.columns]\n",
    "        df_temp.reset_index(inplace=True)\n",
    "        df_temp.rename({'index': 'user'}, inplace=True, axis=1)\n",
    "        \n",
    "        df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['ip', 'ip_3']:    \n",
    "    df_temp = df_trans.groupby(['user'])[f].agg([\n",
    "        ('trans_{}_count'.format(f), 'count')\n",
    "    ]).reset_index()\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['hour', 'trans_day_diff']:\n",
    "    df_temp = df_trans.groupby('user')[f].agg([\n",
    "        ('trans_{}_mean'.format(f), 'mean'), \n",
    "        ('trans_{}_std'.format(f), 'std'),\n",
    "    ]).reset_index()\n",
    "    df_feature = df_feature.merge(df_temp, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\n",
    "        'balance', 'balance_avg', 'balance1', 'balance1_avg', 'balance2',\n",
    "        'balance2_avg', 'product1_amount', 'product2_amount',\n",
    "        'product3_amount', 'product4_amount', 'product5_amount', 'product6_amount'\n",
    "]:\n",
    "    df_feature[f] = df_feature[f].apply(lambda x: int(\n",
    "        x.split(' ')[1]) if type(x) != float else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.82it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 185.66it/s]\n"
     ]
    }
   ],
   "source": [
    "cate_features = ['sex', 'provider', 'level', 'verified', 'regist_type', 'agreement1', 'agreement2', 'agreement3', 'agreement4', 'province', 'city', 'service3', \n",
    "                 'service3_level']\n",
    "\n",
    "for f1 in tqdm(cate_features):\n",
    "    for f2 in cate_features:\n",
    "        df_feature['{}_{}'.format(f1, f2)] = df_feature[f1] + '_' + df_feature[f2]\n",
    "        \n",
    "for f in tqdm(cate_features):\n",
    "    df_feature['{}_cnt'.format(f)] = df_feature.groupby([f])['user'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:08<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "dense_features = ['age', 'using_time', 'card_a_cnt', 'card_b_cnt', 'card_c_cnt', 'card_d_cnt', 'op1_cnt', 'op2_cnt', 'service1_cnt', 'service1_amt', 'service2_cnt', \n",
    "                  'agreement_total', 'acc_count', 'login_cnt_period1', 'login_cnt_period2', 'ip_cnt', 'login_cnt_avg', 'login_days_cnt', 'balance', 'balance_avg', \n",
    "                  'balance1', 'balance1_avg', 'balance2', 'balance2_avg', 'product1_amount', 'product2_amount', 'product3_amount', 'product4_amount', 'product5_amount',\n",
    "                 'product6_amount', 'product7_cnt', 'product7_fail_cnt']\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "df_feature[dense_features] = min_max.fit_transform(df_feature[dense_features].values)\n",
    "\n",
    "for f1 in tqdm(dense_features):\n",
    "    for f2 in dense_features:\n",
    "        if f1 != f2:\n",
    "            df_feature['{}_add_{}'.format(f1, f2)] = df_feature[f1] + df_feature[f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature['product7_fail_ratio'] = df_feature[\n",
    "    'product7_fail_cnt'] / df_feature['product7_cnt']\n",
    "df_feature['card_cnt'] = df_feature['card_a_cnt'] + df_feature[\n",
    "    'card_b_cnt'] + df_feature['card_c_cnt'] + df_feature['card_d_cnt']\n",
    "\n",
    "df_feature['acc_card_ratio'] = df_feature['acc_count'] / df_feature['card_cnt']\n",
    "df_feature['login_cnt'] = df_feature['login_cnt_period1'] + \\\n",
    "    df_feature['login_cnt_period2']\n",
    "\n",
    "df_feature['login_cnt_period2_login_cnt_ratio'] = df_feature['login_cnt_period2'] / \\\n",
    "    df_feature['login_cnt']\n",
    "df_feature['login_cnt_period1_login_cnt_ratio'] = df_feature['login_cnt_period1'] / \\\n",
    "    df_feature['login_cnt']\n",
    "\n",
    "df_feature['using_time_op2_cnt_ratio'] = df_feature['using_time'] / \\\n",
    "    df_feature['op2_cnt']\n",
    "df_feature['using_time_op1_cnt_ratio'] = df_feature['using_time'] / \\\n",
    "    df_feature['op1_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欺诈率\n",
    "def stat(df, df_merge, group_by, agg):\n",
    "    group = df.groupby(group_by).agg(agg)\n",
    "\n",
    "    columns = []\n",
    "    for on, methods in agg.items():\n",
    "        for method in methods:\n",
    "            columns.append('{}_{}_{}'.format('_'.join(group_by), on, method))\n",
    "    group.columns = columns\n",
    "    group.reset_index(inplace=True)\n",
    "    df_merge = df_merge.merge(group, on=group_by, how='left')\n",
    "\n",
    "    del (group)\n",
    "    gc.collect()\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def statis_feat(df_know, df_unknow):\n",
    "    df_unknow = stat(df_know, df_unknow, ['province'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['city'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, [\n",
    "                     'city', 'level'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['op_device'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, [\n",
    "                     'age', 'op_device'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['using_time'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, [\n",
    "                     'city', 'op_device'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['age', 'city'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, [\n",
    "                     'op_device', 'level'], {'label': ['mean']})\n",
    "\n",
    "    return df_unknow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_feature[~df_feature['label'].isnull()]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_feature[df_feature['label'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stas_feat = None\n",
    "kf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "for train_index, val_index in kf.split(df_train, df_train['label']):\n",
    "    df_fold_train = df_train.iloc[train_index]\n",
    "    df_fold_val = df_train.iloc[val_index]\n",
    "\n",
    "    df_fold_val = statis_feat(df_fold_train, df_fold_val)\n",
    "    df_stas_feat = pd.concat([df_stas_feat, df_fold_val], axis=0)\n",
    "\n",
    "    del (df_fold_train)\n",
    "    del (df_fold_val)\n",
    "    gc.collect()\n",
    "\n",
    "df_test = statis_feat(df_train, df_test)\n",
    "df_feature = pd.concat([df_stas_feat, df_test], axis=0)\n",
    "df_feature = df_feature.reset_index(drop=True)\n",
    "\n",
    "del (df_stas_feat)\n",
    "del (df_train)\n",
    "del (df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(cate_features):\n",
    "    for f2 in dense_features:\n",
    "        df_feature['{}_{}_mean'.format(f, f2)] = df_feature.groupby([f])[f2].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47782, 4253)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the generate features\n",
    "df_feature.to_csv(\"data_2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
